# Ollama API base URL (no trailing slash).
# Use a different host/port if Ollama runs elsewhere (e.g. remote or Docker).
OLLAMA_BASE_URL=http://localhost:11434

# Timeout in seconds for Ollama HTTP requests (chat, generate, embed, etc.).
# Increase for large models or slow hardware (e.g. 300).
OLLAMA_TIMEOUT=120

# Read timeout for streaming (chat/generate with stream=true). Defaults to 3Ã— OLLAMA_TIMEOUT if unset.
# OLLAMA_STREAM_READ_TIMEOUT=360

# Log level: DEBUG, INFO, WARNING, ERROR.
# OLLAMA_MCP_LOG_LEVEL=INFO
